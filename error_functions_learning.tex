%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%\documentclass[pdflatex,sn-mathphys]{sn-jnl}
\documentclass[pdflatex,referee,sn-mathphys]{sn-jnl}

%% \documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
%%\documentclass[iicol,pdflatex,sn-mathphys]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%% <additional latex packages if required can be included here>
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{color, colortbl}
\usepackage{paralist}
%%%% 

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\newcommand{\ie}{\emph{i.e.}}
\newcommand{\cp}{\textsc{CP}\xspace}
\newcommand{\csp}{\textsc{CSP}\xspace}
\newcommand{\cop}{\textsc{COP}\xspace}
\newcommand{\efsp}{\textsc{EFSP}\xspace}
\newcommand{\efop}{\textsc{EFOP}\xspace}
\newcommand{\cfn}{\textsc{CFN}\xspace}
\newcommand{\cbls}{\textsc{CBLS}\xspace}
\newcommand{\ghost}{\textsc{GHOST}\xspace}
\newcommand{\cppn}{\textsc{CPPN}\xspace}
\newcommand{\icn}{\textsc{ICN}\xspace}

\newcommand{\cproblem}[3]%
{\begin{trivlist}
  \item[]%
    \textbf{Problem:} \textsc{#1}\\
    \textit{Input:} #2\\
    \textit{Question:} #3
  \end{trivlist}%
}

\newcommand{\efopmodel}[4]%
{\begin{trivlist}
  \item[]%
    \textbf{Variables:} #1\\
    \textbf{Domains:} #2\\
    \textbf{Constraints:} #3\\
    \textbf{Objective function:} #4
  \end{trivlist}%
}

\definecolor{Gray}{gray}{0.9}
\newcolumntype{g}{>{\columncolor{Gray}}l}

\newcommand{\flo}[1]{\textcolor{blue}{\bf Flo: \xspace #1}}
\newcommand{\jf}[1]{\textcolor{red}{\bf JF: \xspace #1}}

\begin{document}

% \title[Interpretable  Error Functions  Learning for  CP]{Interpretable
%   Error Functions Learning for Constraint Programming}

\title[Automatic Error Function Learning with ICN]{Automatic Error Function Learning with Interpretable Compositional Networks}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Florian} \sur{Richoux}}\email{florian@richoux.fr}
\author[2]{\fnm{Jean-François} \sur{Baffier}}\email{jf@baffier.fr}

\affil*[1]{\orgname{AIST}, \orgaddress{\city{Tokyo}, \country{Japan}}}
\affil[2]{\orgname{IIJ}, \orgaddress{\city{Tokyo}, \country{Japan}}}

%\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

\abstract{   In  Constraint   Programming,  constraints   are  usually
  represented  as predicates  allowing or  forbidding combinations  of
  values.    However,   some   algorithms    can   exploit   a   finer
  representation: error functions.  By  associating a function to each
  constraint type to evaluate the quality of an assignment, it extends
  the    expressiveness    of    regular    Constraint    Satisfaction
  Problem/Constrained  Optimization Problem  formalisms.  Their  usage
  comes with a  price though: it makes  problem modeling significantly
  harder, since users  must provide a set of error  functions that are
  not  always  easy   to  define.   Here,  we  propose   a  method  to
  automatically learn an error function corresponding to a constraint,
  given  its predicate  version only.   This is,  to the  best of  our
  knowledge, the first attempt  to automatically learn error functions
  for hard constraints. In this paper, we also give for the first time
  a formal definition of  combinatorial problems with hard constraints
  represented  by error  functions.  Our  method aims  to learn  error
  functions in  a supervised fashion,  trying to reproduce  either the
  Hamming or the  Manhattan distance, by using a graph  model we named
  Interpretable Compositional  Networks.  This model allows  us to get
  interpretable   results.   We   run  experiments   on  7   different
  constraints  to show  its  versatility.  Experiments  show that  our
  system can  learn functions that  scale to high dimensions,  and can
  learn fairly  good functions over  incomplete spaces.  We  also show
  that learned  error functions can  be used efficiently  to represent
  constraints in different classic problems.}

\keywords{Combinatorial Satisfaction, Combinatorial Optimization, Constraint Programming, Problem Modeling, Error Function, Interpretable Learning}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec:introduction}

Twenty   years  separate   Freuder's  papers   \cite{Freuder1997}  and
\cite{Freuder2018},  both   about  the  grand   challenges  Constraint
Programming  (\cp)  must  tackle  \emph{``to  be  pioneer  of  a  new
  usability    science     and    to    go    on     to    engineering
  usability''}~\cite{Freuder2007}.

To  respond  to   the  lack  of  a  ``Model  and   Run''  approach  in
\cp~\cite{Puget2004,Wallace2003},   several    languages   have   been
developed  since  the  late 2000's,  such  as  ESSENCE~\cite{essence},
XCSP~\cite{XCSP-paper}  or  MiniZinc~\cite{minizinc}.   However,  they
require users to have deep expertise on global constraints and to know
how well  these constraints, and  their associated mechanisms  such as
propagators,  are suiting  the  solver.   We are  still  far from  the
original Holy Grail  of \cp: \emph{``the user states  the problem, the
  computer solves it''}~\cite{Freuder1997}.%% Some progress has been
%% made though, like in automatic problem modeling~\cite{Freuder2018}:
%% one can cite, among other  works, constraint detection described in
%% a natural language~\cite{Kiziltan2016},  automatic constraint model
%% production    from    an    abstract    constraint    specification
%% language~\cite{AMJFH2011}, and  acquiring constraint  networks from
%% examples classified by the user~\cite{Bessiere2017}.

This paper makes a contribution  in automatic \cp problem modeling. We
focus  on Error  Function  Satisfaction and  Optimization Problems  we
defined  in  the  next   section.   Compare  to  classical  Constraint
Satisfaction  and Constrained  Optimization Problems,  they rely  on a
finer structure about  the problem: the cost  functions network, which
is an  ordered structure over  invalid assignments (in our  case) that
some  solvers,  such as  constraint-based  local  search solvers,  can
exploit efficiently to improve the search.

In  this  paper,  we  propose   a  method  to  learn  error  functions
automatically; a direction that, to the best of our knowledge, had not
been explored in Constraint Programming.

\section{Error Function Satisfaction and Optimization Problems}\label{sec:efsp}

Constraint  Satisfaction Problem  (\csp) and  Constrained Optimization
Problem (\cop) are constraint-based  problems defined upon a classical
hard constraint network, where constraints can be seen as predicates
allowing or forbidding some combinations of variable assignments.

Likewise,  Error  Function  Satisfaction  Problem  (\efsp)  and  Error
Function  Optimization Problem  (\efop) are  constraint-based problems
defined upon a specific hard constraint network named cost function
network~\cite{GuidedTour}        or        semi-ring        constraint
network~\cite{handbookCP}.   Both  networks  are  equivalent  for  the
purpose of  this paper: cost  function networks exactly  correspond to
semi-ring   constraint   networks   with  a   totally   ordered   cost
structure~\cite{GuidedTour}.

Constraints    are     then    represented    by     cost    functions
\(f: D_1  \times D_2 \times  \ldots \times D_n \rightarrow  E\), where
\(D_i\) is  the domain of  \(i\)-th variable in the  constraint scope,
\(n\) the number of variables (\ie,  the size of this scope) and \(E\)
the set of possible costs.

A cost function network is a quadruplet \(\langle V, D, F, S \rangle\)
where \(V\) is a  set of variables, \(D\) the set  of domains for each
variable, \ie,  the sets of values  each variable can take,  \(F\) the
set of cost functions and \(S\)  a cost structure. A cost structure is
also a quadruplet \(S = \langle  E, \oplus, \bot, \top \rangle\) where
\(E\)  is the  totally ordered  set  of possible  costs, \(\oplus\)  a
commutative,  associative,  and   monotone  aggregation  operator  and
\(\bot\)  and  \(\top\) are  the  neutral  and absorbing  elements  of
\(\oplus\), respectively.

In  Constraint Programming,  cost  functions are  often associated  to
soft constraints: they can be interpreted as preferences over valid or
acceptable assignments.  However, this is not necessarily the case: it
actually depends on  the cost structure.  For  instance, the classical
cost structure
\[S_{t/f} = \langle \{true, false\}, \wedge, true, false\rangle\] make
the  cost  function  network  equivalent  to  a  classical  constraint
network, so dealing with hard constraints.

Here,  we  consider  particular  cost functions  that  also  represent
hard constraints only, by considering the additive cost structure
\(S_+ = \langle \mathbb{R}, +,  0, \infty\rangle\).  The additive cost
structure produces  useful cost  function networks  capturing problems
such as Maximum Probability Explanation (MPE) in Bayesian networks and
Maximum    A   Posteriori    (MAP)   problems    in   Markov    random
fields~\cite{Hurley16}.

We  name {\bf  error  function}  a cost  function  defined  in a  cost
function   network   with   the   additive   cost   structure~\(S_+\).
Intuitively,  error  functions  are  preferences  over  \emph{invalid}
assignments.   Let  \(f_c\)  be   an  error  function  representing  a
constraint \(c\)  and \(\vec x_c\)  an assignment of variables  in the
scope of \(c\).  Then \(f_c(\vec x_c) = 0\) iff \(\vec x_c\) satisfies
the  constraint  \(c\).   For all  invalid  assignments~\(\vec  i_c\),
\(f_c(\vec i_c) > 0\) such that  the closer \(f_c(\vec i_c)\) is to 0,
the closer~\(\vec i_c\) is to satisfy \(c\).

The goal  of this paper  is not to study  the advantages of  such cost
function   networks   over    regular   constraint   networks.    Some
Constraint-Based Local Search methods  such as Adaptive Search exploit
this  structure  efficiently  and show  state-of-the-art  experimental
results,     both     in     sequential~\cite{AS}     and     parallel
solving~\cite{caniou2015constraints}.  Such  question would  deserve a
deep investigation which  is out of the scope of  this paper. However,
we can  give a short  illustration of  the advantage of  cost function
networks over regular constraint networks. Figure~\ref{fig:landscapes}
shows  the search  landscapes of  the same  constraint network  from a
regular constraint  network (Figure~\ref{fig:csp_landscape})  and cost
function network (Figure~\ref{fig:efsp_landscape})  point of view. The
network  is  composed  of   the  constraints  AllDifferent\((x,  y)\),
\(x   \leq   y\)   and   \(x+2y=6\).    Error   functions   used   for
Figure~\ref{fig:efsp_landscape} have been learned  with our system. We
can see that  the \csp landscape is mostly composed  of large plateaus
with an error  measure (the number of violated  constraints) between 0
and 2.   On the other  hand, the \efsp  landscape is more  convex with
slopes  toward the  solution, with  a broader  scope of  error values,
between 0 and 6, allowing  richer comparisons of variable assignments.

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
    \includegraphics[width=0.8\linewidth]{./csp_landscape_complex_zero_big}
		\caption{\csp landscape}\label{fig:csp_landscape} 
	\end{subfigure}
  \hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
    \includegraphics[width=0.8\linewidth]{./efsp_landscape_complex_zero_big}
		\caption{\efsp landscape}\label{fig:efsp_landscape} 
	\end{subfigure}
  \caption{Search landscapes of a small constraint network.}
  \label{fig:landscapes}
\end{figure}

The  term  ``error   function''  has  been  used   in  the  Constraint
Programming literature in the same sense as in this paper.  Borning et
al.~\cite{Borning94} are the  first, to the best of  our knowledge, to
use this  term. It also  appears in the constraint-based  local search
literature,  like in  Codognet et  al.~\cite{AS} describing  the local
search algorithm  Adaptive Search.   We can  also find  the equivalent
term   ``penalty   function''~\cite{Galinier04}   for   local   search
algorithms in  Constraint Programming.  However, penalty  function can
also refers to functions representing soft constraints in Mathematical
Programming~\cite{MezuraMontes2011}.   Therefore, to  avoid confusions
with cost functions or penalty functions for soft constraints, we
opted for the name ``error function''.

Let \(\vec  x\) be a variable  assignment, and denote by  \(\vec x_c\)
the  projection  of \(\vec  x\)  over  variables  in  the scope  of  a
constraint \(c\).  We can now define the \efsp and \efop problems.

\cproblem%
{Error Function Satisfaction Problem}%
{ A cost function network \(\langle V, D, F, S_+ \rangle\).}%
{ Does   a   variable   assignment   \(\vec   x\)   exist   such   that
  \(\forall f_c \in F,\ f_c(\vec x_c)=0\) holds?}%

\cproblem%
{Error Function Optimization Problem}%
{ A  cost function  network \(\langle  V, D,  F, S_+  \rangle\) and  an
  objective function \(o\).}%
{ Find a  variable assignment \(\vec  x\) maximizing or  minimizing the
  value        of        \(o(\vec         x)\)        such        that
  \(\forall f_c \in F,\ f_c(\vec x_c)=0\) holds.}%

Thanks to their constraint structure,  problems modeled by an \efsp or
an  \efop can  be solved  by  some solvers  faster than  if they  were
modeled by a  \csp or a \cop,  as shown by results  of Experiment~3 in
Section~\ref{subsubsec:results_xp3}.  Another way to consider it: with
the  same computation  budget, a  solver could  solve larger  \efsp or
\efop problems. However, we do not  obtain this gain for free: this is
a trade  with modeling simplicity.  Indeed,  it is not always  easy to
find good error functions to describe constraints.

This paper focuses on this  ``easy-to-use'' problem and proposes a way
to  automatically  learn error  functions.   Users  provide the  usual
constraint  network  \(\langle V,  D,  C  \rangle\), and  our  systems
computes      the      equivalent     cost      function      networks
\(\langle V, D, F, S_+  \rangle\). Learned functions composing the set
\(F\) are independent of the number of variables in constraints scope,
and are expressed in an  interpretable way: users can understand these
functions and  easily modify them at  will.  This way, users  can have
the power of \efsp and \efop with the same modeling effort as for \csp
and \cop.

\section{Related works}\label{sec:related_works}

This  work  belongs to  one  of  the  three directions  identified  by
Freuder~\cite{Freuder2007}:   \emph{Automation},   \ie,   ``automating
efficient and  effective modeling and  solving''.  To the best  of our
knowledge, few efforts have been done on the modeling side.

Another  of  these  three  directions which  is  slightly  related  is
\emph{Acquisition} described  by Freuder to be  ``acquiring a complete
and correct representation of  real problems''.  Remarkable efforts on
this topic  have been done  by Bessiere's research team,  for instance
with  constraints learning  by  induction from  positive and  negative
examples~\cite{Bessiere05},   with   interactive  queries   asked   to
users~\cite{Bessiere07},  and with  constraint  network learning  also
through with interactive queries~\cite{Bessiere13}.

Model Seeker~\cite{Beldiceanu12}  is a passive learning  system taking
positive  examples  only, which  are  certainly  easier for  users  to
provide.   It transforms  examples  into data  adapted  to the  Global
Constraint   Catalog~\cite{catalog},   then  generate   and   simplify
candidates by eliminating dominated ones. Model Seeker is particularly
efficient  to find  a good  inner structure  of the  target constraint
network.

Teso~\cite{Teso19}  gives a  good survey  on Constraint  Learning with
this  interesting remark:  ``A major  bottleneck of  [constraint-based
problem  modeling] is  that obtaining  a formal  constraint theory  is
non-obvious: designing an appropriate, working constraint satisfaction
or optimization  problem requires both domain  and modeling expertise.
For this reason, in  many cases a modeling expert is  hired and has to
interact with domain expert to  acquire informal requirements and turn
them into  a valid constraint  theory.  This process can  be expensive
and time-consuming.''

We can  consider that Constraint Acquisition,  or Constraint Learning,
focuses on modeling expertise and puts domain expertise on background:
users  would not  be able  to understand  and modify  a learned  model
without the  help of a modeling  expert. The goal of  these systems is
mainly to simplify the interaction between the domain and the modeling
experts.

Our  work  is  taking  the  opposite direction:  we  focus  on  domain
expertise and put modeling expertise  on background.  With our system,
users always have the  control over constraints' representation, which
can  be  modified  at  will  to fit  needs  related  to  their  domain
expertise.   \emph{Constraint Implementation  Learning}  is what  best
describes this research topic.

\section{Method design}\label{sec:method_design}

The main result of this paper  is to propose a method to automatically
learn an error function representing  a constraint, to make easier the
modeling of \efsp/\efop.   We are tackling a  regression problem since
the  goal is  to find  a function  that outputs  a target  value. More
specifically,  looking for  the  model of  a function  in  a space  of
mathematical representation  is a  symbolic regression  problem.  Such
problems  are often  handled by  Genetic Programming  methods, but  we
explain  in Section~\ref{subsec:learning}  why we  do not  use Genetic
Programming to learn error functions in this work.

In this paper, we denote by \textbf{method} the methodology we propose
to learn error function, and  by \textbf{system} the implementation of
our method.  Before diving into the description of our method, we need
to introduce some essential notions.

\subsection{Definitions}\label{subsec:definitions}

We propose a method to automatically  learn an error function from the
\emph{concept}  of   a  constraint.   As  described   in  Bessiere  et
al.~\cite{Bessiere2017},  the \textbf{concept}  of a  constraint is  a
Boolean  function  that,  given  an  assignment  \(\vec  x\),  outputs
\emph{true} if  \(\vec x\) satisfies the  constraint, and \emph{false}
otherwise. Concepts  are the  predicate representation  of constraints
referred at the beginning of Section~\ref{sec:efsp}.

Our method learns  error functions in a  supervised fashion, searching
for  a  function  computing  either the  \emph{Hamming  cost}  or  the
\emph{Manhattan cost} of each assignment. The \textbf{Hamming cost} of
an  assignment  \(\vec x\)  is  the  minimum  number of  variables  in
\(\vec x\)  to reassign  to get a  \textbf{solution}, \ie,  a variable
assignment satisfying  the considered constraint.  In  other words, it
is the Hamming  distance from \(\vec x\) to its  closest solution.  If
\(\vec  x\)  is  a  solution,  then   its  Hamming  cost  is  0.   The
\textbf{Manhattan cost}  of an  assignment \(\vec  x\) is  the minimum
number of  value incrementations or  decrementations in \(\vec  x\) to
perform to get  a solution.  It corresponds to  the Manhattan distance
from \(\vec x\)  to its closest solution. As for  the Hamming cost, if
\(\vec x\) is a solution, its Manhattan cost is 0.

Given the  number of variables of  a constraint and their  domain, the
\textbf{constraint   assignment  space}   is   the   set  of   couples
\((\vec  x, b)\)  where  \(\vec x\)  is an  assignment  and \(b\)  the
Boolean output of the concept  applied on \(\vec x\).  Such constraint
assignment spaces  can be  generated from  concepts. These  spaces are
said to be \textbf{complete} if and  only if they contain all possible
assignments, \ie, all combinations of  possible values of variables in
the scope  of the constraint,  given their domain.   Otherwise, spaces
are said to be \textbf{incomplete}.

In  this work,  we consider  an error  function to  be a  (non-linear)
combination of elementary operations.  Complete spaces are intuitively
good training sets  since it is easy to compute  the exact Hamming and
Manhattan costs of their elements.   We also consider assignments from
incomplete spaces  where their Hamming  and Manhattan costs  have been
approximated  regarding  a  subset  of  solutions  in  the  constraint
assignment space, in case their exact formulas are unknown.

\subsection{Main result}\label{subsec:main_result}

Our  method learns  error  functions as  a  non-linear combination  of
elementary   operations    and   lies    on   a   model    we   called
\textbf{Interpretable  Compositional Network}  (\icn).  An  \icn is  a
directed acyclic  multipartite graph composed of  $k$ independent sets
organised  in such  a  way that  vertices of  an  independent set  are
exclusively connected to  all vertices of a unique  independent set in
the graph. Thus, if we abstract each independent set by a vertex, then
we obtain a directed line.

\(\icn\)s are  networks in the  graph theory  way, \ie, a  graph where
vertices or arcs  have attributes.  In an \icn,  vertices are actually
elementary operations,  and arcs  represent the  possible combinations
between these elementary operations.

In its  shape, an \icn looks  like a neural network:  the hierarchical
organisation  between  independent  sets exactly  corresponds  to  the
hierarchical structure of  layers of neurons. Due  to this similarity,
we call  \emph{layers} the independent  sets within an  \icn. However,
the similarity  with neural networks  stops here: the  main difference
between \(\icn\)s and neural networks  it that there are no activation
functions  within  \(\icn\)s (the  elementary  operation  of a  vertex
cannot  be considered  to be  an activation  function). A  second main
difference is  that \(\icn\)s do  not have weights on  arcs connecting
two vertices.   We first introduced \(\icn\)s  in a poster paper  as a
variant of neural  networks~\cite{richoux2021gecco}.  Although \icn is
deeply inspired from a variant of  neural networks, the correct way to
interpret our  model is  to see  it as  a particular  directed acyclic
graph.

In this paper, our \(\icn\) model  is composed of four layers, each of
them having a specific purpose and  composed of vertices with a unique
operation for each. Learning an error function boils down to selecting
which vertices  of the  network would  be part  of the  composition of
elementary operations.

Here is our method workflow in 4 points:
\begin{enumerate}
\item     Users    provide     a     regular    constraint     network
  \(\langle  V, D,  C  \rangle\)  where \(C\)  is  a  set of  concepts
  representing constraints.
\item For  each constraint concept  \(c\), we generate its  \icn input
  space \(X\),  which is  either a  complete or  incomplete constraint
  assignment space.  Those input spaces are our training sets.  If the
  space  is complete,  then the  Hamming and  Manhattan costs  of each
  assignment  can  be pre-computed  before  learning  our \icn  model.
  Otherwise,  the  incomplete  space  is composed  of  randomly  drawn
  assignments and only an approximation of their Hamming and Manhattan
  costs can be pre-computed.
\item We learn the composition of elementary operations modeled by our
  \icn in a supervised fashion, with the following loss function:
  \begin{equation}\label{eq:loss}
    loss = Min\left(Hamming_{diff}, Manhattan_{diff}\right) + Regularization
  \end{equation}
  with 
  \begin{align}
    Hamming_{diff} &= \frac{\sum_{\vec x \in X} \mid ICN(\vec x) -
                       Hamming(\vec x)\mid}{n} \nonumber \\
    Manhattan_{diff} &= \frac{\sum_{\vec x \in X} \mid ICN(\vec x) -
                         Manhattan(\vec  x)\mid}{n~\times (\mid  D\mid - 1)} \nonumber \\
    Regularization &= 0.9  \times  \frac{\text{Number  of  selected  vertices}}{\text{Total number of vertices}}
  \end{align}
  where \(X\) is the constraint assignment space, \icn(\(\vec x\)) the
  output  of the  \icn model  giving  \(\vec x  \in X\)  as an  input,
  Hamming(\(\vec  x\))  and  Manhattan(\(\vec  x\))  respectively  the
  pre-computed  Hamming  and  Manhattan  costs  of  \(\vec  x\)  (only
  approximated  if \(X\)  is  incomplete), and  a regularization  term
  between  0 and  0.9  to  favor short  \(\icn\)s,  \ie,  with as  few
  elementary  operations  as  possible.   To make  a  fair  comparison
  between the Hamming and Manhattan costs, we normalize these costs in
  \([0,1]\) by dividing  the Hamming difference with  the number \(n\)
  of variables  in the scope  of the  constraint, and by  dividing the
  Manhattan  difference with  \(n\) times  the difference  between the
  maximal  value and  the minimal  value  in the  domain of  variables
  (corresponding  to the  cardinality of  the  domain minus  1 in  our
  context).
\item We have hold-out test sets of assignments from larger dimensions
  to evaluate the quality of our learned error functions.
\end{enumerate}

% Notice we also have a hold-out validation set to fix the values of our
% hyperparameters, as described in Section~\ref{sec:ga}.

% Notice we  do not have  any validation sets  since we use  the default
% value of the parameters.

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{./model_nn}
  \caption{Our 4-layer network. Layers with blue vertices have mutually exclusive operations.}
  \label{fig:model}
\end{figure}

Figure~\ref{fig:model}   is   a   schematic  representation   of   our
network. It  takes as input an  assignment of \(n\) variables,  \ie, a
vector    of    \(n\)    integers.    The    first    layer,    called
\textbf{transformation  layer},  is   composed  of  18  transformation
operations, each of  them applied element-wise on each  element of the
input vector.   This element-wise computation property  is what allows
our model to be able to compute  the error function of a constraint of
an arbitrary size,  \ie, with an arbitrary number of  variables in the
constraint  scope.  This  is  a very  powerful  feature  that  enables
learning  error functions  over a  small constraint  assignment spaces
that scale to larger spaces.

Such transformation  operations are  for instance the  maximum between
the  \(i\)-th and  \(i+1\)-th elements  of  the input  vector, or  the
number of  \(j\)-th elements of  the vector smaller than  the \(i\)-th
element such  that \(j >  i\) holds.  This  layer is composed  of both
linear and  non-linear operations.   If an  operation is  selected, it
outputs a vector of \(n\) integers.

\begin{example}
  Consider one of our  18 transformation operations: ``\emph{Number of
    $x[j]$ such that  \(j < i\) and $x[i]=x[j]$},''  with \(x[i]\) and
  \(x[j]\)  respectively  the  \(i\)-th  and  \(j\)-th  value  of  the
  assignment \(\vec x\).  Giving the assignment \((3,1,3,4,3,1,2)\) as
  input,   this   transformation    operation   outputs   the   vector
  \((0,0,1,0,2,1,0)\).
\end{example}

If \(k\) transformation  operations are selected, then  the next layer
gets \(k\)  vectors of  \(n\) integers  as input.   This layer  is the
\textbf{arithmetic layer}.  Its  goal is to apply  a simple arithmetic
operation in a  component-wise fashion on all \(i\)-th  element of our
\(k\)  vectors  to get  one  vector  of  \(n\)  integers at  the  end,
combining  previous  transformations into  a  unique  vector. We  have
considered  only  2  arithmetic   operations:  the  addition  and  the
multiplication.

\begin{example}
  Consider the addition as the arithmetic operation, and as inputs the
  two  vectors  \((0,0,1,0,2,1,0)\)   and  \((2,0,1,0,2,0,0)\).  Then  the
  arithmetic layer outputs the vector \((2,0,2,0,4,1,0)\).
\end{example}

The output of the arithmetic layer is given to the \textbf{aggregation
  layer}. This layer crunches the  whole vector into a unique integer.
At  the moment,  the aggregation  layer is  composed of  2 operations:
\emph{Sum} computing the sum of input values and \emph{\(Count_{>0}\)}
counting the number of input values strictly greater than 0.

\begin{example}
  Consider  the aggregation  operation \emph{\(Count_{>0}\)}  applied on
  \((2,0,2,0,4,1,0)\). Then, the aggregation  layer outputs \(4\), since 4
  values in the input are strictly greater than 0.
\end{example}

Finally, the computed scalar  is transmitted to the \textbf{comparison
  layer}  with 9  operations.  Examples  of these  operations are  the
identity,  or  the   absolute  value  of  the  input   minus  a  given
parameter. This  layer compares its  input with an  external parameter
value, or the number of variables  of the problem, or the domain size,
among others.

\begin{example}
  Consider             the            comparison             operation
  \emph{\(Max(0\mathrel{,}\  input -  parameter)\)}.   Assume that  we
  have the parameter \(p=1\) and  \(4\) as input. The comparison layer
  outputs \(3\).
\end{example}

All elementary operations  in our model are generic: we  do not choose
them to fit one or  several particular constraints.  The complete list
of elementary operations in our \icn model is given in appendix.

Although  an in-depth  study of  the elementary  operations properties
would be interesting, this is out of the scope of this paper: its goal
is to show  that learning interpretable error functions  via a generic
\icn is possible. There is no reason  to reduce \icn to its current 31
elementary operations  or even a 4-layer  architecture.  Such elements
can be changed by users to best fit their needs.

To  have  simple   models  of  error  functions,   operations  of  the
arithmetic, the  aggregation, and  the comparison layers  are mutually
exclusive, meaning that  precisely one operation is  selected for each
of  these layers.   However, many  operations from  the transformation
layer can be selected to compose the error function. This allows us to
have  a very  comprehensible combination  of elementary  operations to
model  an error  function, making  it readable  and intelligible  by a
human being.  For instance, the  most frequenly learned error function
for                           AllDifferent                          is
\(Count_{>0}\big(\#\{j\ \mid\ x[j]=x[i] \text{ and } j>i\}\big)\), and
\(\mid  \big(\sum^{n}_{i=1} x[i]\big)  - p\mid\)  for LinearSum,  with
\(p\) the  right hand side  constant of  the equation. Thus,  once the
model of  an error function is  learned, users have the  choice to run
the network in  a feed-forward fashion to compute  the error function,
or to re-implement  it directly in a programming  language.  Users can
use our  system to  find error functions  automatically, but  they can
also  use it  as a  decision support  system to  find promising  error
functions that they may modify and adapt by hand.

\subsection{Learning \efsp/\efop models through an \efop model}\label{subsec:learning}

As written in the introduction of Section~\ref{sec:method_design}, our
error function  learning problem is  a symbolic regression  problem, a
family of problems usually tackled through Genetic Programming.

The issue  with Genetic Programming is  that, in addition of  having a
huge search space of  mathematical representations, we cannot guaranty
learning error  functions that  are independent  from the  input size,
\ie,  from  the  number  of  variables in  the  scope  of  the  target
constraint. This property is mandatory  to have a unique function able
to compute the error of a given  constraint on 3, 30 or 300 variables.
This property  also allows  us to quickly  learn error  functions over
small constraint scopes  in order to use them for  computing the error
of their constraints over large scopes.

What we  do in this work  is actually solving a  \emph{biased symbolic
  regression problem}, where  the bias is induced by  the structure of
the \icn we provide.  The architecture of an \icn gives a common shape
to  error  functions  and  drastically reduces  the  search  space  of
mathematical representations.  More importantly, it enforces the input
size independence property.

In  our a  first  work,  we used  genetic  algorithms  to learn  error
functions  via \(\icn\)s~\cite{richoux2021gecco}.  In  this paper,  we
present  a more  efficient  and  elegant way  to  learn  them: we  can
actually model the problem of  learning error functions of \efsp/\efop
models as an \efop.

Indeed,  learning  an  error  function  via  an  \icn  corresponds  to
selecting the right vertices in such a way that it both satisfies some
conditions and minimizes the difference  with the Hamming or Manhattan
cost  on the  constraint  assignment  space used  as  a training  set.
Therefore,   learning   an   error  function   is   simultaneously   a
combinatorial optimization  problem itself,  and a  regression problem
that   can  be   tackled   by  supervised   learning,   as  shown   in
Section~\ref{subsec:main_result}.

This  combinatorial  optimization  problem   can  be  modeled  by  the
following \efop model:%
\efopmodel%
{One variable for each vertex in the \icn.}%
{Boolean  domain \(\{0,1\}\)  for  each  variable, representing  their
  selection.}%
{Mutual exclusion,  No empty layer, Parameter-specific  operations. We
  explain these contraints below.}%
{Loss function given by Equation~\ref{eq:loss}.}%

The ``Mutual exclusion''  constraint is applied on the  three last layers,
\ie, the  arithmetic, the  aggregation and  the comparison  layers. It
ensures  that  exactly   one  vertex  in  each  of   these  layers  is
selected.  This simply corresponds to the linear equation \(\sum_{v \in layer} v = 1\).
We use \(\mid  1 - \sum_{v \in  layer} v \mid\) as  the error function
for this contraint.

The ``No empty layer'' constraint only concerns the first layer, where
at least  one vertex need to  be selected. This can  be represented by
the  linear inequation  \(\sum_{v \in  layer} v  \geq 1\)  and can  be
represented by the error function \(Max(0, 1 - \sum_{v \in layer} v)\).

Some  elementary operations  within our  \icn involve  the value  of a
parameter \(p\)  in their  computation. Indeed, some  constraints need
some parameters  to be defined, such  as the linear equation  with the
constant   at   the   right   hand  side   of   the   equation.    The
``Parameter-specific   operations''  constraint   enforces  elementary
operations to be  part of the operation combination if  and only if we
are dealing with  a constraint involving one or  some parameters. This
constraint is easily  expressed as follows: considering  that \(m\) is
the number  of selected  parameter-specific elementary  operations, if
the  target constraint  contains some  parameters, then  we must  have
\(m \geq  1\), otherwise \(m =  0\) must hold. The  error function for
this constraint is then a combination of both previous error functions
regarding if the target constraint  contains at least one parameter or
not.

Modeling  and  solving  this  \efop  model has  been  done  using  the
framework \ghost~\cite{GHOST}.   Notice that besides our  training and
tests sets,  we do not have  validation sets simply because  we do not
performed any parameter tweaking: we use the solver of \ghost with its
default parameter values for all experiments in this paper.



\section{Attempts and failures}\label{sec:failures}

We  strongly  believe  exposing  explored  directions,  attempts,  and
reasons for their  failure can be highly beneficial  to the scientific
community. Before showing our experimental  results, we sum up in this
short section our  principal attempts and failures  to represent error
functions before coming up with a representation through Interpretable
Compositional Networks.

\subsection{Series of sinusoids}\label{subsec:sinusoids}

Let $f$  be a error function  we aim to  learn. Our first idea  was to
represent error functions as a sum of $p$ sinusoids, such that
\begin{displaymath}
  f(\vec{x}) := \sum_{k=0}^{p}\big(a_k * cos(\vec{x}.2\pi.k) + b_k * sin(\vec{x}.2\pi.k)\big)
\end{displaymath}
Thus, learning $f$ boiled down to learning coefficients $a_i, b_i$.

Although any functions,  even nonperiodic ones, can  be represented by
such a sum, error functions we  want to learn might be too complicated
and too high dimensional to be  easily expressed by a reasonably small
sum of  sinusoids.  Moreover,  such a representation  of $f$  does not
allow  to  express  relations  among variables  in  $\vec  x$,  unlike
\icn. Furthermore, even if we could learn such a representation of $f$
in  a small  dimensional  space, it  was unclear  how  to extend  this
function to higher dimensions.

We tried two approaches to learn $a_i, b_i$ coefficients: multivariate
interpolation  and genetic  algorithms. 

\paragraph{Interpolation}   We  tried   different   tools  for   doing
multivariate    interpolation    such    as    \textsc{chebpol}    and
\textsc{splinter}, but it didn't lead to satisfying results, even when
we sampled  10\% of the constraint  assignment space, which is  a huge
part of an incomplete space.

\paragraph{Genetic Algorithm} We start with a population of 100 random
individuals,  where  an  individual  is  the  vector  of  real  values
$(a_0, b_0,  \ldots, a_p,  b_p)$.  Those  coefficient are  assigned to
random  values from  a  normal  distribution of  mean  0 and  standard
deviation $\sqrt  n$, with $n$ the  number of variables of  the target
constraint.   The  fitness  function  was to  maximize  the  empirical
correlation length  multiplied by the  mean of  $f$ on samples  from a
random walk  such that  $f(\vec{x}) \neq  0$.  The idea  was to  get a
function $f$ with a low ruggedness (in other words, a smooth function)
such that its value concerning non-solutions is high.

The ruggedness of  a landscape can be computed by  doing a random walk
from a random assignment, and compute the empirical correlation length
$l$ (see  Hoss and Stützle~\cite{Hoos2005},  Chapter 5). To do  so, we
need first to define the empirical autocorrelation function $r(i)$.
\begin{displaymath}
  r(i)               :=\frac{\frac{1}{(m-i)}\cdot\sum\limits_{k=1}^{m-i}\left(f_{k}-\overline{f}\right)
    \cdot\left(f_{k+i}-\overline{f}\right)}{\frac{1}{m}\cdot\sum\limits_{k=1}^{m}\left(f_{k}-\overline{f}\right)^{2}}
\end{displaymath}
with $m$ the length of the random  walk, $f_k$ the value of $f$ on the
$k$-th assignment of  the random walk, and $\overline{f}$  the mean of
those $f_k$.

We can then compute the empirical correlation length $l$ such that
\begin{displaymath}
  l := \frac{1}{ln(|r(1)|)}
\end{displaymath}

Intuitively, the higher the value $l$, the smoother the function $f$.

However, we needed to avoid  learning a flat function projecting every
assignment to the value 0 (it would be very smooth but also completely
useless for the solver). To avoid this situation, the fitness function
of the  genetic algorithm was  to maximize $l$  times the mean  of $f$
over  assignments in  the  random  walk that  are  not solutions  (ie,
$f(\vec{x})  \neq  0$).   Thus,  the algorithm  was  supposed  to  try
learning   smooth  functions   that  severely   penalize  non-solution
assignments.

These   two  approaches,   multivariate   interpolation  and   genetic
algorithms, failed mainly because of the same reason: expressing error
functions with a sum of $p$ sinusoids led to either functions far from
being satisfying  ($p < 40$)  or with  too many coefficients  to learn
($p \geq 40$).

\subsection{\cppn}

Before  expressing  error  functions   with  \(\icn\)s,  we  tried  to
represent  them   with  a  Compositional   Pattern-Producting  Network
(\cppn)~\cite{CPPN}, a variant of  neural networks from which \(\icn\)
is inspired.  Our \cppn architecture  was a two-layer network composed
of units with an activation  function among the identity, the absolute
value, the sine function, the hyperbolic tangent, the cubic hyperbolic
tangent, a sigmoid function, and a Gaussian function.

\paragraph{\efop} We  first tried  to tackle  the problem  of learning
error  functions represented  by a  \cppn as  an optimization  problem
modeled  as an  \efop.  We  have  one variable  for each  unit in  the
network to express  its selection (or its absence) to  model the error
function. Domains are  then binary. A constraint  assured that outputs
of  the \cppn  given some  assignments as  input are  greater than  or
equals to  the Hamming distance  of these assignments. To  have smooth
functions,  we also  considered the  objective function  based on  the
computation  of the  landscape  ruggedness, as  introduced in  Section
\ref{subsec:sinusoids} above.   However, such an \efop  model revealed
itself  to  be  inefficient  due  to  a  unique  constraint  over  all
variables, which was quite artificial and  led to a poor (actually, an
absent)  constraint  network that  neither  a  complete solver  nor  a
constraint-based local  search can  exploit.  Moreover,  computing the
ruggedness as an objective function to have smooth functions tended to
output very flat functions.

We  also  felt  that  activation  functions of  our  \cppn  where  too
arbitrate to express error functions. We  then had the idea to replace
these activation functions with more meaningful ones in the context of
comparing  and  evaluating values  in  a  assignment, leading  to  our
current Interpretable Compositional Networks.

\section{Experiments}\label{sec:experiments}

To show the versatility of our method, we tested it on seven different
constraints:   AllDifferent,   Ordered,   LinearSum,   LinearLessThan,
LinearGreaterThan,  NoOverlap1D,  and   Minimum.   According  to  XCSP
specifications~\cite{XCSP-paper}\footnote{see                     also
  \url{http://xcsp.org/specifications}},   those  global   constraints
belong  to  four  different  families:  Comparison  (AllDifferent  and
Ordered),      Counting/Summing       (LinearSum,      LinearLessThan,
LinearGreaterThan),  Packing/Scheduling  (NoOverlap1D) and  Connection
(Minimum).  Again according to  XCSP specifications, these constraints
are among the  twenty most popular and common constraints.   We give a
brief description of those seven constraints below:

\begin{itemize}
  \item \textbf{AllDifferent} ensures  that variables must all  be assigned
  to different values.
  \item \textbf{LinearSum}       ensures       that      the       equation
  \(x_1 + x_2 +  \ldots + x_n = p\) holds, with the  parameter \(p\) a given
  integer.
\item    \textbf{LinearLessThan}   ensures    that   the    inequation
  \(x_1 + x_2 + \ldots + x_n  \leq p\) holds, with the parameter \(p\)
  a given integer.
\item   \textbf{LinearGreaterThan}   ensures   that   the   inequation
  \(x_1 + x_2 + \ldots + x_n  \geq p\) holds, with the parameter \(p\)
  a given integer.
  \item \textbf{Minimum} ensures  that the  minimum value of  an assignment
  verifies  a given  numerical condition.  In this  paper, we  choose to
  consider that  the minimum value must  be greater than or  equals to a
  given parameter \(p\).
  \item \textbf{NoOverlap1D}  is considering  variables as  tasks, starting
  from a  certain time (their  value) and each  with a given  length \(p\)
  (their  parameter).   The  constraint   ensures  that  no   tasks  are
  overlapping,  \ie, for  all indexes  \(i,j  \in \{1,n\}\)  with \(n\)  the
  number   of   variables,  we   have   \(x_i   +   p_i  \leq   x_j\)   or
  \(x_j + p_j  \leq x_i\). To have  a simpler code, we  have considered in
  our system that all tasks have the same length \(p\).
\item  \textbf{Ordered} ensures  that an  assignment of  \(n\) variables
  \((x_1\mathrel{,} \ldots\mathrel{,}  x_n)\) must  be ordered,  given a
  total  order.  In  this paper,  we  choose the  total order  \(\leq\).
  Thus,  for  all   indexes  \(i,j  \in  \{1,n\}\),  \(i   <  j\)  implies
  \(x_i \leq x_j\).
\end{itemize}

\subsection{Experimental protocols}\label{subsec:experimental_protocols}

We conducted three experiments, with  two of them requiring samplings.
These samplings have been done  using Latin hypercube sampling to have
a good diversity  among drawn assignments.  We  draw assignments until
we get \(k\) solutions and \(k\) non-solutions.

Due to  stochastic learning, all  learning and testing have  been done
100 times.  We did not re-run  batches of experiments to keep the ones
with  the best  results, as  it should  always be  the case  with such
experimental protocols.

All experiments have been  done on a computer with a  Core i9 9900 CPU
and 32 GB of RAM, running on Ubuntu 20.04. Programs have been compiled
with GCC with  the 03 optimization option. Our entire  system, its C++
source code, experimental setups, and the results files are accessible
on
GitHub\footnote{\url{https://github.com/richoux/LearningErrorFunctions/tree/2.1} }.

\subsubsection{Experiment 1: scaling}\label{subsubsec:xp1}

The goal  of this experiment is  to show that learned  error functions
scale to  high-dimensional constraints, indicating that  learned error
functions are independent of the size of the constraint scope.

For  this  experiment,  error  functions are  learned  upon  a  small,
complete constraint assignment space, composed of about 500\(\sim\)600
assignments  and containing  about 10\(\sim\)20\%  of solutions.   For
each constraint, we run 100  error function learning over pre-computed
complete constraint assignment space.  Then, we compute the test error
of these  learned error  functions over a  sampled test  set.  Sampled
test sets contain 10,000 solutions  and 10,000 non-solutions, with 100
variables on domains of size 100, belonging to a constraint assignment
space  of size  \(100^{100}~=~10^{200}\) (except  for NoOverlap1D  and
Ordered, as explained below), thus greatly larger than training spaces
containing 500\(\sim\)600 assignments.

For  AllDifferent,  LinearSum, LinearLessThan,  LinearGreaterThan  and
Minimum, it is easy to define by hand a function computing the Hamming
and Manhattan  costs of any  assignment \(\vec x\)  without generating
the  whole constraint  assignment  space.  For  these constraints,  we
tested the corresponding  error function on spaces  with 100 variables
and domains of size 100.

Whereas for Ordered  and NoOverlap1D, since these  two constraints are
intrinsically combinatorial,  finding a  function computing  the exact
Hamming  and  Manhattan  costs  of  any  assignment  is  not  trivial.
Therefore,  we sampled  10,000 solutions  and 10,000  non-solutions in
constraint assignment spaces of Ordered  with 12 variables and domains
of    size    18    (so   \(18^{12}\)    assignments,    \ie,    about
\(1.15e^{15}\)) and NoOverlap1D  with 10 variables and domains
of size 35 (\(35^{10}  \simeq 2.75e^{15}\) assignments).  Then
we approximate the  Hamming and Manhattan costs  of each non-solution,
considering the  closest solution among the  10,000 sampled solutions.
It was not possible to build  test sets of higher dimensions for these
two constraints  since sampling  10,000 solutions is  challenging: for
Ordered, we  estimate the solution  rate to be  \(8.6e^{-10}\)
(to make  this number concrete,  after 100 billion samplings,  one can
expend finding  86 solutions); for  NoOverlap1D, the solution  rate is
about \(3.6e^{-9}\).  On a regular  computer, it took us a bit
more than 10 hours to generate  the test set of Ordered.  Knowing that
such an  execution time grows  exponentially, generating test  sets of
higher dimensions would take an unreasonable amount of time.

We show  normalized mean training  and test errors: first,  we compute
the mean  error among  all assignments composing  the training  or the
test set. Normalization is done like in Equation~\ref{eq:loss}: errors
from error functions approximating the Hamming cost are divided by the
number of variables  composing the assignments, and  errors from error
functions approximating the  Manhattan cost are divided  by the number
of variables composing the assignments times to difference between the
maximal and the minimal value in the domains.

Concidering  normalized errors  is important:  for instance,  having a
Hamming-like mean error of 5 on  assignments with 10 variables and 100
variables  is significantly  different:  looking  at their  normalized
error allows to realized that the first one implies a mean error every
2 variables, the second a mean error every 20 variables.

\subsubsection{Experiment 2: learning over incomplete spaces}\label{subsubsec:xp2}

If, for any reasons, it is not possible to build a complete constraint
assignment  space, a  robust system  must be  able to  learn effective
error functions upon large, incomplete  spaces where the exact Hamming
and Manhattan costs of their assignments is unknown.

In this experiment,  we built pre-sampled training  spaces by sampling
10,000  solutions   and  10,000  non-solutions  on   large  constraint
assignment  spaces of  size between  \(10^{12}\) and  \(10^{13}\), and
with  solution  rates  from  \(0.15\)\%  to  \(2e^{-7}\)\%.  Then,  we
approximate the  Hamming and Manhattan  costs of each  non-solution by
computing  their  Hamming and  Manhattan  distances  with the  closest
solution among the 10,000 ones,  learn error functions on these 20,000
assignments and their estimated Hamming and Manhattan costs.  Like for
Experiment~1, we run 100 error functions learning of these pre-sampled
incomplete spaces, so  that each learning relies on  the same training
set.  Finally, we  evaluate the learned error functions  over the same
test sets than Experiment~1.

\subsubsection{Experiment 3: learned error functions to solve problems}\label{subsubsec:xp3}

The goal of  this experiment is to assess that  learned error function
can effectively be used to  solve 3 classic combinatorial problems. We
modeled   these  problems   and   solve  them   using  the   framework
\ghost~\cite{GHOST}.

We  consider  the  mean  and  median  run-time  to  compare  different
representations of our  constraints.  We take as baseline  a pure \csp
model where  constraints are  predicates.  We  also consider  an \efsp
model with an efficient  hand-crafted error function for AllDifferent.
We compare  those with two  models using error functions  learned with
our  system: a.,  our \efsp  model using  the most  frequently learned
error function from  the previous experiments and  computed by running
the \icn graph in a fast-forward fashion, and b., our \efsp model with
the same  error function but  directly hard-coded in C++.   The solver
and its  parameters remain the  same: the  only thing that  changes in
these  four different  models  is the  expression  of the  constraint.
Notice that for LinearSum, % LinearLessThan and
% LinearGreaterThan
our system  learned the canonical  and optimal error function,  so the
hand-crafted and hard-coded versions are the same.

All  problem instances  have been  solved  100 times  each, using  the
constraint-based local  search solver within the  framework \ghost, in
sequential mode.  For each run, we set a timeout of 60 seconds.  If no
solutions have been found within 60 seconds, we consider the run to be
unsolved.

\paragraph{Sudoku} Sudoku is a puzzle  game where the player must fill
a grid with numbers from 1 to  \(n\), \(n\) being the size of the side
of the grid,  such that all numbers  in the same row,  the same column
and the same sub-square must be different.  Sudoku can be modeled as a
satisfaction problem  using the AllDifferent constraint  only.  We run
100   resolutions   of   random   \(9\times9\),   \(16\times16\)   and
\(25\times25\) Sudoku grids.

\paragraph{Magic Square} Magic Square is a $n \times n$ grid that must
be filled up with  all number from 1 to $n^2$  (thus, all numbers must
appear exactly once in the grid), such  that the sum of each row, each
column, and the two diagonals must be equal to a constant $c$.  We can
avoid using  the AllDifferent  constraint by  randomly filling  up the
grid with  all expected numbers and  ask the solver to  find a correct
permutation.   The constraints  over the  rows, columns,  and the  two
diagonal  are modeled  with  LinearSum  since the  value  of $c$  only
depends on  $n$ and  is known to  be $c  = n(n^2 +  1)/2$. We  run 100
resolutions of \(25\times25\) and \(30\times30\) Magic Square grids.


\paragraph{Killer Sudoku} Killer  Sudoku is the same as  Sudoku but in
such a way that  the grid is paved with blocks  of cells, named cages,
usually composed  of 2,  3, or  4 cells.  Each  cage is  associated an
integer, and the sum of numbers in their cells must be equals to their
integer. A  killer Sudoku  instance starts with  an empty  grid, cages
preventing from  trivial solutions. AllDifferent constraints  are used
to model the  regular Sudoku rules of this puzzle  game, and LinearSum
constraints   are  modeling   cages.   We  run   100  resolutions   of
a \(9\times9\) Killer Sudoku grid.

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.3\linewidth}
		\centering
    \includegraphics[width=\linewidth]{./sudoku}
		\caption{Sudoku \(9\times9\)}\label{fig:sudoku} 
	\end{subfigure}
  \hfill
	\begin{subfigure}[t]{0.3\linewidth}
		\centering
    \includegraphics[width=\linewidth]{./magic_square}
		\caption{Magic Square \(3\times3\)}\label{fig:magic_square} 
	\end{subfigure}
  \hfill
	\begin{subfigure}[t]{0.3\linewidth}
		\centering
    \includegraphics[width=\linewidth]{./killer_sudoku}
		\caption{Killer Sudoku \(9\times9\)}\label{fig:killer_sudoku} 
	\end{subfigure}
  \caption{Our classic combinatorial problems as benchmarks.}
  \label{fig:problems}
\end{figure}

% \paragraph{k-Vertex Cover}  k-Vertex Cover is the  decision version of
% the  graph problem  Vertex Cover:  given a  graph, one  must select  a
% subset of vertices such that all edges  in the graph have at least one
% vertex  from  this  subset  as   an  endpoint.   Vertex  Cover  is  an
% optimization problem where  the goal is to find the  minimal subset of
% vertices  with  this  property.  The k-Vertex  Cover  problem  is  the
% decision  version where  the goal  is  to check  if such  a subset  of
% cardinality at most k exists.  k-Vertex Cover can be modeled using the
% two constraints LinearLessThan for checking that we do not select more
% than k  vertices, and  LinearGreaterThan to make  sure that  all edges
% have at least one of their endpoints among selected vertices.
% \flo{Taille d'instance}

\subsection{Results}\label{subsec:results}

In this part, we denote by \(n\) the number of variables, \(d\) the domain
size, and \(p\) the value of a possible parameter. Constraint instances
are denoted by \emph{name-n-d[-p]}.

\subsubsection{Experiments 1 \& 2}\label{subsubsec:results_xp1}

Tables~\ref{tab:training_complete}   and~\ref{tab:training_incomplete}
show the  training errors of Experiments~1  and~2, respectively, where
error functions have been learned  100 times for each constraint.  The
first column contains  the normalized mean training error  of the most
frequently  learned  error  function  among the  100  runs,  with  its
frequency in parenthesis.   Next columns concern the  median, the mean
and the standard deviation.

Learning an error function is done  quickly: we set a timeout of 200ms
to learn an error function  over complete constraint assignment spaces
from  Experiment~1,  and  30s over  incomplete  constraint  assignment
spaces from Experiment~2.  Learnings have been done  in parallel using
16 threads.

\begin{table}[h]
  \begin{center}
    \begin{minipage}{270pt}
      \caption{Training errors (100 runs)  of Experiment~1, over small and
        complete constraint assignment spaces.}\label{tab:training_complete}
      \centering
      \begin{tabular}{@{}g|g|g|g|g@{}}
        \toprule
        \rowcolor{white}
        Constraints & most freq & median & mean & std dev \\
        \midrule
        \rowcolor{white}
        AllDifferent-4-5 & 0 ~~~~~(100)& 0 & 0 & 0 \\
        LinearSum-3-8-12 & 0 \,~~~~~~(74)& 0 & 0.005 & 0.014 \\
        \rowcolor{white}
        LinearLessThan-3-8-12 & 0 \,~~~~~~(85)& 0 & 0.008 & 0.021 \\
        LinearGreaterThan-3-8-12 & 0 \,~~~~~~(92)& 0 & 0.003 & 0.012 \\
        \rowcolor{white}
        Minimum-4-5-3 & 0 \,~~~~~~(82)& 0 & 0.005 & 0.016 \\
        NoOverlap1D-3-8-2 & 0.007 ~(34)& 0.007 & 0.009 & 0.003\\
        \rowcolor{white}
        Ordered-4-5 & 0.009 ~(80)& 0.009 & 0.013 & 0.010\\
        \botrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table}

\begin{table}
  \begin{center}
    \begin{minipage}{270pt}
      \caption{Training errors (100 runs)  of Experiment~2, over large and
        incomplete constraint assignment spaces.}\label{tab:training_incomplete}
      \centering
      \begin{tabular}{@{}g|g|g|g|g@{}}
        \toprule
        \rowcolor{white}
        Constraints & most freq & median & mean & std dev \\
        \midrule
        \rowcolor{white}
        AllDifferent-12-12 & 0.018 ~~(91)& 0.018 & 0.019 & 0.001 \\
        LinearSum-12-12-42 & \(2e^{-4}\) \,~~(63)& \(2e^{-4}\)& 0.015 & 0.031\\
        \rowcolor{white}
        LinearLessThan-12-12-42 & \(4e^{-4}\) \,~~(50)& 0.005 & 0.022 & 0.033 \\
        LinearGreaterThan-12-12-42 & 0.027 ~~(46)& 0.034 & 0.032 & 0.004 \\
        \rowcolor{white}
        Minimum-12-12-6 & 0.019 ~~(24)& 0.021 & 0.022 & 0.004 \\
        NoOverlap1D-8-32-3 & 0.030 ~~(19)& 0.030 & 0.029 & 0.002\\
        \rowcolor{white}
        Ordered-12-12 & 0.019 ~~(99)& 0.019 & 0.019 & 0.001\\
        \botrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table}

\noindent
Table~\ref{tab:test} contains the normalized mean test errors of error
functions learned  with Experiments~1 and~2, with  their median, mean
and standard  deviation. The  normalized mean test  error of  the most
frequently  learned  error  function   for  each  constraint  in  each
experiment  has  been isolated  in  the  first  column of  number,  for
comparison.

\begin{table}
  \begin{center}
    \begin{minipage}{\linewidth}
      \caption{Test errors (100 runs) in high dimensions of error functions learned
        with Experiments~1 and~2.}\label{tab:test}
      % \small
      \centering
      \begin{tabular}{@{}c|g|g|g|g|g@{}}
        \toprule
        \rowcolor{white}
        Exp. & Constraints & most freq & median & mean & std dev \\
        \midrule
        \rowcolor{white}
        \multirow{7}{*}{1} &
                             AllDifferent-100-100 & 0 & 0 & ~~0 & ~~~0 \\
             &LinearSum-100-100-5279 & 0 & 0 & ~~0.001 & ~~~0.004 \\
        \rowcolor{white}
             &LinearLessThan-100-100-5279 & 0 & 0 & 19.790 & 117.033 \\
             &LinearGreaterThan-100-100-5279 & 0 & 0 & ~~0.003 & ~~~0.007 \\
        \rowcolor{white}
             &Minimum-100-100-30 & 0 & 0 & ~~0.026 & ~~~0.123 \\
             &NoOverlap1D-10-35-3 & 0.062 & 0.062 & ~~0.060 & ~~~0.004\\
        \rowcolor{white}
             &Ordered-12-18 & 0.035 & 0.035 & ~~0.045 & ~~~0.022\\
        \midrule
        \rowcolor{white}
        \multirow{7}{*}{2} &
                             AllDifferent-100-100 & 0.006 & 0.006 & ~~0.007 & ~~~0.001 \\
             &LinearSum-100-100-5279 & 0 & 0 & ~~0.186 & ~~~0.659 \\
        \rowcolor{white}
             &LinearLessThan-100-100-5279 & 0 & 0.001 & ~~0.224 & ~~~0.692 \\
             &LinearGreaterThan-100-100-5279 & 0.020 & 0.020 & ~~0.020 & ~~~\(1e^{-4}\) \\
        \rowcolor{white}
             &Minimum-100-100-30 & 0.200 & 0.200 & ~~0.208 & ~~~0.100 \\
             &NoOverlap1D-10-35-3 & 0.050 & 0.041 & ~~0.041 & ~~~0.004\\
        \rowcolor{white}
             &Ordered-12-18 & 0.037 & 0.037 & ~~0.045 & ~~~0.022\\
        \botrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table}

\noindent
Comparing  Table~\ref{tab:training_complete} with  the  first half  of
Table~\ref{tab:test} lead  us to conclude  that our system is  able to
learn  error   functions  that  scale  for   most  constraint,  namely
AllDifferent,   LinearSum,   LinearLessThan,   LinearGreaterThan   and
Minimum. Our system  has been able to find the  exact Hamming distance
or Manhattan distance for these constraints.

Observe that LinearLessThan has a  surprizingly high mean and standard
deviation. This  is due to  an error  function that show  an excellent
score on the training set but a very poor score on the test sets.  Our
system was  overfitting this  training set twice  within 100  runs. In
practice, this problem could be  alleviated by letting more than 200ms
to the solver to find an optimal solution, by taking a slightly larger
training set or eventually by running a second time the error function
learning, if  the user realises  that it  performs poorly on  the test
set. Since the  learning is done very quickly (as  well as testing the
error function), this is  not a major issue for the  real usage of our
system.

Althought not perfect,  results are good for  NoOverlap1D and Ordered,
which  are clearly  the most  intrinsically combinatorial  constraints
among our seven ones. Our system is able to learn error functions with
a low test errors and a  low standard deviation of performance between
the different learned functions.  However, since their training errors
are significantly lower  than their test errors, one  could think that
our system is overfitting here.   Results from Experiment~2 lead us to
another conclusion.

First, let's  analyse the results  of our five first  constraints over
large, incomplete  training sets. It  is important to stress  that the
real Hamming  and Manhattan costs  in these training sets  are unknown
and  roughtly   approximated  on  purpose.    Nevertheless,  comparing
Table~\ref{tab:training_incomplete}   with   the    second   half   of
Table~\ref{tab:test}  shows  us  that  our  system  is  able  to  find
high-quality    error   functions    for   AllDifferent,    LinearSum,
LinearLessThan and LinearGreaterThan. This illustrates that our system
can  learn   efficient  error  function  over   incomplete  constraint
assignment spaces.  Observe  that the learned error  functions for the
Minimum  constraint over  incomplete spaces  perform poorly,  while it
learn a  perfect error function  over complete spaces.  The  reason is
the  following:  over  complete  spaces, learned  error  function  for
Minimum   reproduce  the   Hamming   cost,  which   is  an   excellent
choice. However,  sampled assignments  that constitute  the incomplete
space guide the solver toward Manhattan-like error functions, which is
a  poor choice  for this  constraint.  This  shows that  learning over
incomplete spaces often produces high-quality error functions, but can
sometimes lead to poorly learned ones.

Let's focus now on NoOverlap1D and Ordered. Looking at their scores on
the first  and second  part of Table~\ref{tab:test},  we can  see that
their  performances  are similar  and  very  homogeneous, with  a  low
standard deviation.  This  is explained but the fact  that their error
functions learned  over both complete  and incomplete spaces  are very
similar, showing  that our system  was not overfitting  their complete
training  sets.  The  reason explaining  the difference  between their
training  errors in  Table~\ref{tab:training_complete}  and the  first
half  of Table~\ref{tab:test}  is because  their training  spaces from
Experiment~1   were  too   small   for   these  highly   combinatorial
constraints,   containing   too   few   different   combinations   and
Hamming/Manhattan cost  patterns. In  other words, those  small spaces
does not  contain very  diverse assignments, penalizing  the learning.
This explains  why results  for NoOverlap1D  are better  over incomplete
spaces than  complete spaces.  However,  we think that  the elementary
operations composing  our \icn model  are not rich enough  to properly
express the complexity of NoOverlap1D and Ordered.

We give  in appendix  the list  of the  most frequently  learned error
function  for  each  constraint,  both over  complete  and  incomplete
spaces.

\subsubsection{Experiment 3}\label{subsubsec:results_xp3}

The goal of this experiment is  not to be state-of-the-art in terms of
run-times for solving  Sudoku, Magic Square and Killer  Sudoku, but to
compare the  average run-times  of the  same solver  on three  or four
nearly  identical  models,  depending  on the  problem,  presented  in
Section~\ref{subsubsec:xp3}.   For models  with  a hand-crafted  error
function of AllDifferent, we  implemented the \emph{primal graph based
  violation error} from Petit  et al.~\cite{Petit2001}.  This function
simply outputs  the number of  couples with identical values  within a
given assignment.  For  LinearSum, we do not  know better hand-crafted
error  functions than  the  one  learned the  most  frequently by  our
system.

To  run this  experiment, we  used the  framework \ghost~\cite{GHOST},
which  includes  a constraint-based  local  search  algorithm able  to
handle both \csp and \efsp models.

\begin{table}
  \begin{center}
    \begin{minipage}{\linewidth}
      \caption{Run-times  in seconds  over 100  runs to  solve classic
        problems with  4 different  representations of  constraints (3
        for Magic Square).\\
        Rows  in  gray   means  that  some  runs   hit  the  60-second
        timeout.}\label{tab:problems}
      % \small
      \centering
      \begin{tabular}{@{}l|c|r|r|r|r@{}}
        \toprule
        Problem & Error  Function & mean & median &  std dev & success (\%)\\
        \midrule
        \multirow{4}{*}{Sudoku \(9\times9\)} &
                                               none (\csp)& \cellcolor{Gray}2.6 & \cellcolor{Gray}0.1 & \cellcolor{Gray}10.6 &\cellcolor{Gray}97\\
                &fast-forward    & \(2e^{-2}\)  & \(2e^{-2}\)  & \(1e^{-2}\) &100\\
                &hard-coded      & \(1e^{-2}\)  & \(1e^{-2}\)  & \(7e^{-3}\) &100\\
                &hand-crafted    & \(1e^{-2}\)  & \(1e^{-2}\)  & \(5e^{-3}\) &100\\
        \midrule
        \multirow{4}{*}{Sudoku \(16\times16\)} &
                                                 none (\csp)& \cellcolor{Gray}59.9 & \cellcolor{Gray}60.0 & \cellcolor{Gray}- &\cellcolor{Gray}1\\
                &fast-forward    & 0.9  & 0.8  & 0.2 &100\\
                &hard-coded      & 0.5 & 0.5 & 0.1 &100\\
                &hand-crafted & 0.4 & 0.3 & 0.1 &100\\
        \midrule
        \multirow{4}{*}{Sudoku \(25\times25\)} &
                                                 none (\csp)& \cellcolor{Gray}- & \cellcolor{Gray}- & \cellcolor{Gray}- &\cellcolor{Gray}0\\
                &fast-forward    & \cellcolor{Gray}28.5  & \cellcolor{Gray}25.7  & \cellcolor{Gray}14.6 &\cellcolor{Gray}94\\
                &hard-coded      & 17.3 & 14.0 & 10.3 &100\\
                &hand-crafted & 9.4 & 7.6 & 5.7 &100\\
        \midrule
        \multirow{4}{*}{Magic Square \(25\times25\)} &
                                                       none (\csp)& 10.4 & 8.5 & 5.5 &100\\
                &fast-forward    & 9.7 & 6.1 & 9.8 &100\\
                &hard-coded      & 6.8 & 5.1 & 5.5 &100\\
        \midrule
        \multirow{4}{*}{Magic Square \(30\times30\)} &
                                                       none (\csp)& \cellcolor{Gray}29.9 & \cellcolor{Gray}27.3 & \cellcolor{Gray}11.4 &\cellcolor{Gray}96\\
                &fast-forward    & \cellcolor{Gray}19.1 & \cellcolor{Gray}16.2 & \cellcolor{Gray}14.3 &\cellcolor{Gray}98\\
                &hard-coded      & 14.9 & 11.7 & 11.5 &100\\
        \midrule
        \multirow{4}{*}{Killer Sudoku \(9\times9\)} &
                                                      none (\csp)& \cellcolor{Gray}- & \cellcolor{Gray}- & \cellcolor{Gray}- &\cellcolor{Gray}0\\
                &fast-forward    & 2.4  & 1.6  & 2.2 &100\\
                &hard-coded      & 1.5 & 1.1 & 1.1 &100\\
                &hand-crafted & 1.1 & 0.8 & 0.9 &100\\
        % \midrule
        % \multirow{4}{*}{Vertex Cover} &
        %                                 none (\csp)& \cellcolor{Gray}- & \cellcolor{Gray}- & \cellcolor{Gray}- &\\
        %        &fast-forward    & 825.61  & 774.36  & 271.09 &\\
        %        &hard-coded      & 537.48 & 539.86 & 162.03 &\\
        \botrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table}

Table~\ref{tab:problems} shows that  \efsp models clearly outperformed
their  equivalent   \csp  model,  except  for   smaller  Magic  Square
instances. We run 100 solving  of different problem instances for each
model and compute the mean and  median run-time in seconds, as well as
the standard deviation  and the success rate, \ie, the  number of runs
out  of 100  that found  a solution  within 60  seconds. Rows  in gray
indicates that the success rate is below 100\%.

We can estimate  the overload of computing the  error function through
the  interpretable  compositional  network   (ran  in  a  feed-forward
fashion), compare to a hard-coded  version of the same error function.
We recall that  one advantage of our method is  to output intelligible
error functions, letting the choice  to users to compute this function
through the  Interpretable Compositional  Network or  to let  them the
possibility     to    code     it     themselves.     Results     from
Table~\ref{tab:problems} show that the overload is such that run-times
of error  functions executed  through the  interpretable compositional
network  are between  30\% and  80\% longer  than run-times  of their
hard-coded version.

The same difference of permormance  is observed between hard-coded and
hand-crafted  version  of  error  functions:  we  see  that  the  most
frequently learned  error function by  our system, once  hard-coded in
C++, finds  solutions within between 30\%  and 80\% more time  that as
the   carefully   hand-crafted   error    function   from   Petit   et
al.~\cite{Petit2001}.    Although  perfectible,   these  results   are
encouraging and show that our method can be used to automatically find
error functions that are usable in practice.

\section{Conclusion}\label{sec:conclusion}

In  this  paper,  we  give  a  formal  definition  of  Error  Function
Satisfaction and  Optimization Problems,  and we  present a  method to
learn   error  functions   automatically   upon  a   model  based   on
Interpretable  Compositional Networks,  a particular  directed acyclic
graph.  To  the best of  our knowledge, this  is the first  attempt to
learn error functions for hard constraints automatically.

We  have  tested   our  system  over  7   different  constraints.   In
Experiment~1 over  small and  complete training  spaces, it  finds the
exact Hamming  or Manhattan costs  for 5 of  them.  The beauty  of our
method is that  we learn error functions by solving  an Error Function
Optimization Problem. Error functions of these 7 constraints have been
learned over  constraint assignment  space composed  of 500\(\sim\)600
assignments and perfectly scale on high-dimension constraint instances
with  \(10^{200}\)   assignments.   In   Experiment~2,  we   show  the
robustness of our  system by learning error  functions over incomplete
constraint   assignment  space   containing   20,000  randomly   drawn
assignments from  spaces of  about \(10^{12}\) assignments.   It finds
high-quality error  functions for 4  out of  5 constraints that  had a
perfect error function in the previous Experiment~1.

With the analysis of our results, we  conclude it is better to use our
system   over  complete   spaces  for   simple  constraints   such  as
AllDifferent, LinearSum  and Minimum.   For more  complex constraints,
like  NoOverlap1D  and  Ordered,  experiments  show  that  very  small
training  spaces are  too  restricted  and do  not  contain enough  of
diverse  assignments, and  the  current set  of elementary  operations
composing our \icn model is certainly not expressive enough.

Results from Experiment~3 show that:
\begin{inparaenum}[1.]
\item at  least while  using a  constraint-based local  search solver,
  there is a  real gain to model Constraint  Programming problems with
  \efsp models rather than the classical \csp models.
\item our system learns high-quality  error functions that can be used
  in  practice  to  efficiently express  contraints  in  combinatorial
  problems.
\end{inparaenum}

These two points  imply that our method allows users  to get the power
of error function-based models for  free, leveraging the difficulty of
their modeling: users can get an \efsp or an \efop model with the same
modeling  effort  as   for  classical  \csp  and   \cop  models.  Like
Freuder~\cite{Freuder2007} wrote:  ``\textit{This research  program is
  not easy  because 'ease of  use' is  not a science}.''   However, we
believe our  result is a step  toward the 'ease of  use' of Constraint
Programming.

One of the  most significant results in this paper  is that our system
outputs interpretable  results.  Error functions output  by our system
are intelligible. This allows our system to have two operating modes:
\begin{inparaenum}[1.]
  \item a  fully automatic system,  where error functions  are learned
    and  called within  our  system, being  completely transparent  to
    users  who  only need  to  furnish  a  concept function  for  each
    constraint, in addition  to the regular sets of  variables V and
    domains D, and
  \item a  decision support system, where  users can look at  a set of
    proposed error functions, pick up and modify the one they prefer.
\end{inparaenum}   

We made this system modular, allowing  users with special needs to add
or  remove operations  in  the  system to  learn  more specific  error
functions.

% The current  limitation of our  system is  that it struggles  to learn
% high-quality error  function for  very combinatorial  constraints.  By
% combining results from  Experiments~1 and~2, we can  conclude that: 1.
% our system  is not  overfitting but need  more diverse  and expressive
% operations   to  learn   a  high-quality   error  function   for  such
% constraints,  and 2.  the Hamming  cost  is certainly  not the  better
% choice to represent their assignment error.

An extension of our work would  be to do reinforcement learning rather
than  supervision learning  based on  the Hamming  or Manhattan  cost.
Indeed, even  if these costs seem  natural metrics to tell  how far an
assignment  is to  be  a solution  for  constraint-based local  search
solvers, it could also be too restrictive.  Learning via reinforcement
learning would allow finding error  functions that are more adapted to
the chosen solver.

Another  interesting  extension  is   the  theoretical  study  of  the
properties of  elementary operations and their  combinations, together
with the properties  of the search landscape  such combinations imply,
such as the ruggedness, the solution density, the presence of funnels,
the  solution symetries,  etc.   Having a  deeper  knowledge of  these
properties would help selecting the right elementary operations for an
\icn regarding the type of constraints users aim to represent.


\section*{Statements and Declarations}

The authors have  no competing interests to declare  that are relevant
to the content of this article.

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
% \item Ethics approval 
% \item Consent to participate
% \item Consent for publication
% \item Availability of data and materials
% \item Code availability 
% \item Authors' contributions
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%


\bibliography{error_functions_learning}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%% Default %%
%%\input sn-sample-bib.tex%


\begin{appendices}

\section{List of elementary operations}

\subsection{Transformation layer}

\begin{itemize}
\item Identity
  \begin{displaymath}
    id(x[i]) := x[i]
  \end{displaymath}

\item Number of elements on the right equals to \(x[i]\)
  \begin{displaymath}
    Count_{=}^{r}(x[i]) := \#\{x[j] \ \mid\  j>i\ \wedge\ x[j]=x[i]\}
  \end{displaymath}

\item Number of elements on the right smaller than \(x[i]\)
  \begin{displaymath}
    Count_{<}^{r}(x[i]) := \#\{x[j] \ \mid\  j>i\ \wedge\ x[j]<x[i]\}
  \end{displaymath}

\item Number of elements on the right greater than \(x[i]\)
  \begin{displaymath}
    Count_{>}^{r}(x[i]) := \#\{x[j] \ \mid\  j>i\ \wedge\ x[j]>x[i]\}
  \end{displaymath}

\item Number of elements on the left equals to \(x[i]\)
  \begin{displaymath}
    Count_{=}^{l}(x[i]) := \#\{x[j] \ \mid\  j<i\ \wedge\ x[j]=x[i]\}
  \end{displaymath}

\item Number of elements on the left smaller than \(x[i]\)
  \begin{displaymath}
    Count_{<}^{l}(x[i]) := \#\{x[j] \ \mid\  j<i\ \wedge\ x[j]<x[i]\}
  \end{displaymath}

\item Number of elements on the left greater than \(x[i]\)
  \begin{displaymath}
    Count_{>}^{l}(x[i]) := \#\{x[j] \ \mid\  j<i\ \wedge\ x[j]>x[i]\}
  \end{displaymath}

\item Number of elements equals to \(x[i]\) + param
  \begin{displaymath}
    Count_{=+p}(x[i]) := \#\{x[j] \ \mid\  x[j] = x[i] + param\}
  \end{displaymath}

\item Number of elements smaller than \(x[i]\) + param
  \begin{displaymath}
    Count_{<+p}(x[i]) := \#\{x[j] \ \mid\  x[j] < x[i] + param\}
  \end{displaymath}

\item Number of elements greater than \(x[i]\) + param
  \begin{displaymath}
    Count_{>+p}(x[i]) := \#\{x[j] \ \mid\  x[j] > x[i] + param\}
  \end{displaymath}

\item Max(0, \(x[i]\) - param)
\item Max(0, param - \(x[i]\))
\item Max(0, \(x[i] - x[i+1]\) )
\item Max(0, \(x[i+1] - x[i]\) )
\item Number of elements equals to \(x[i]\)
  \begin{displaymath}
    Count_{=}(x[i]) := \#\{x[j] \ \mid\  x[j] = x[i]\}
  \end{displaymath}

\item Number of elements smaller than \(x[i]\)
  \begin{displaymath}
    Count_{<}(x[i]) := \#\{x[j] \ \mid\  x[j] < x[i]\}
  \end{displaymath}

\item Number of elements greater than \(x[i]\)
  \begin{displaymath}
    Count_{>}(x[i]) := \#\{x[j] \ \mid\  x[j] > x[i]\}
  \end{displaymath}

\item Number of elements greater than or equals to \(x[i]\) AND less than or equals to \(x[i]\) + param 
  \begin{displaymath}
    Count_{>=<+p}(x[i]) := \#\{x[j] \ \mid\ x[j] \geq x[i] \wedge x[j] \leq x[i] + param\}
  \end{displaymath}

\end{itemize}

\subsection{Arithmetic layer}

\begin{itemize}
\item Sum of the \(i\)-th element of each vector \(\vec x_j\): \(\forall i \in \{1,n\} \sum_{j=1}^k x_j[i]\)
\item Product of the \(i\)-th element of each vector \(\vec x_j\): \(\forall i \in \{1,n\} \prod_{j=1}^k x_j[i]\)
\end{itemize}

\subsection{Aggregation layer}

\begin{itemize}
\item \(\sum_{i=1}^n x[i]\)
\item \(Count_{>0}(\vec x) := \#\{x[i] \mid x[i] > 0\}\)
\end{itemize}

\subsection{Comparison layer}

\begin{itemize}
\item \(id(x) = x\)
\item \(\mid x - param\mid \)
\item \(Max( 0, param - x )\)
\item \(Max( 0, x - param )\)
\item \(Euclidian_p(x)\) := If(x = param) then 0 else 1 + \(\frac{\mid x - param\mid }{maximal\ domain\ size}\)
\item \(Euclidian(x)\) := If(x = 0) then 0 else 1 + \(\frac{x}{maximal\ domain\ size}\)
\item \(\mid\) x - number of variables\(\mid\)
\item Max( 0, number of variables - x )
\item Max( 0, x - number of variables )
\end{itemize}

\section{Most frequently learned error functions}

Elementary  operations  from  the  transformation  layer  are  applyed
element-wise. Therefore,  an operation like  \(Count_{=}^{l}(\vec x)\)
is  applied  on  each  element  \(x[i]\) of  the  vector  \(\vec  x\),
producing a transformed vector.

We  denote by  \(n\), \(d\)  and \(p\)  the number  of variables,  the
domain size and the value of the parameter, respectively.

\subsection{Complete spaces}

\begin{equation*}
  \begin{array}{ll}
    AllDifferent: &\(Count_{>0}\big( Count_{=}^{l}(\vec x)\big)\)\\
    LinearSum: &\(\mid\sum_{i=1}^n x[i] - p\mid\)\\
    LinearLessThan: &\(Max( 0, \sum_{i=1}^n x[i] - p )\)\\
    LinearGreaterThan: &\(Max( 0, p - \sum_{i=1}^n x[i] )\)\\
    Minimum: &\(Count_{>0}\big(Max(0, p - x[i])\big)\)\\
    NoOverlap1D: &\(Max\Big( 0, \sum_{i=1}^n\left(Count_{=}^{l}(\vec x) + Count_{<+p}(\vec x)\right) - n \Big)\)\\
    Ordered: &\(\sum_{i=1}^n\big(Max(0, x[i] - x[i+1] )\big)\)\\
  \end{array}
\end{equation*}

\subsection{Incomplete spaces}

\begin{equation*}
  \begin{array}{ll}
    AllDifferent: &\(Count_{>0}\big( Count_{=}(\vec x) + Count_{=}^{r}(\vec x)\big)\)\\
    LinearSum: &\(\mid\sum_{i=1}^n x[i] - p\mid\)\\
    LinearLessThan: &\(Max( 0, \sum_{i=1}^n x[i] - p )\)\\
    LinearGreaterThan: &\(Max\Big( 0,  p - \sum_{i=1}^n\big(id(\vec x) \times Max(0, x[i+1] - x[i] )\big) \Big)\)\\
    Minimum: &\(Max\Big( 0,  \sum_{i=1}^n\big(Count_{>+p}(\vec x) + Max(0, p - x[i] )\big) - p\Big)\)\\
    NoOverlap1D: &\(Max\Big( 0, \sum_{i=1}^n\left(Count_{>=<+p}(\vec x) + Count_{<+p}(\vec x)\right) - p \Big)\)\\
    Ordered: &\(\sum_{i=1}^n\big(Count_{<}^{r}(\vec x)\big)\)\\
  \end{array}
\end{equation*}

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

\end{document}
